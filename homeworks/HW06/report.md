# HW06 – Report

> Файл: `homeworks/HW06/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Dataset

- Какой датасет выбран: `S06-hw-dataset-02.csv`
- Размер: (18000 строк, 39 столбцов)
- Целевая переменная `target`: Класс 0 - 73,7%, Класс 1 - 26,3%
- Признаки: 37 числовых признаков

## 2. Protocol

- Разбиение на train/test в соотношении 80/20, random state = 42
- CV на train: 5 разбиений, оптимизировлся ROC-AUC
- ОЦенка производилась по метрикам accuracy, F1 и ROC-AUC

## 3. Models

В ходе работы сравнивались следующие модели:

- DummyClassifier (strategy=most_frequent)
- LogisticRegression (с подбором гиперпараметра C)
- DecisionTreeClassifier (с подбором гиперпараметров `max_depth` и `min_samples_leaf`)
- RandomForestClassifier (с подбором гиперпараметра `max_depth`)
- AdaBoost с использованием решающих пней (с подбором гиперпараметров `learning_rate` и `n_estimators`)

## 4. Results

Таблица метрик на test по всем моделям:

| accuracy |       f1 |   roc_auc |               model |
|----------|----------|-----------|---------------------|
| 0.892222 | 0.760198 | 0.928733  |        RandomForest |
| 0.826389 | 0.654887 | 0.832561  |        DecisionTree |
| 0.829167 | 0.606526 | 0.832221  |            AdaBoost |
| 0.811944 | 0.560675 | 0.797694  |      LogReg(scaled) | 
| 0.737500 | 0.000000 | 0.500000  |Dummy(most_frequent) |

- По метрике ROC-AUC лучшей моделью оказался RandomForest.

## 5. Analysis

- Модели `DesicionTree` и `RandomForest` достаточно устойчивы к изменению random state, при 5 разных прогонах метрика ROC-AUC существенно не изменялась
- Для лучшей модели confusion matrix имеет следующий вид:
```
[[2597, 58],
[330, 615]]
```

- По графику `importance.png` видно, что наибольшую важность имеет признак f16, затем гораздо меньшую важность имеет признак f01. Все остальные признаки имеют важность гораздо меньше, чем f16.

## 6. Conclusion

В ходе выполнения данной работы мы выяснили, что использование ансамблей решающих деервьев может значительно повысить точность предсказаний. Решающие деревья склонны к переобучению, поэтом мы искуственно ограничиваем их глубину. Для выбора наилучшей модели для данной задачи мы использовали честный ML-протокол: создали бейслайны, обучили разные модели и сравнили метрики моделей между собой и с бейслайами. 
